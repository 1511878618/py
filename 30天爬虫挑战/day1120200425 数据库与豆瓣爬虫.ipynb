{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正文\n",
    "需要学习re和class类的定义\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import re \n",
    "import collections\n",
    "import math \n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class comment(object):\n",
    "    \"\"\"\n",
    "    关于评论的对象\n",
    "    参数：\n",
    "    stars: 推荐程度\n",
    "    content：评论的内容 \n",
    "    time ：评论时间 \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,stars,content,Time,vote):\n",
    "        self.stars = stars\n",
    "        self.content = content\n",
    "        self.Time = Time\n",
    "        self.vote = vote \n",
    "    \n",
    "    def getContent(self):\n",
    "        return content\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tv(object):\n",
    "    \"\"\"\n",
    "    一个以电视剧作为对象的类  \n",
    "    参数：\n",
    "    Dict：指豆瓣爬虫返回的JSON对象，解析化后获得的dict\n",
    "    属性： \n",
    "    title：电视剧题目 \n",
    "    rank：电视剧评分 \n",
    "    information：电视剧的主演等介绍 \n",
    "    introduction：电视剧的剧情简介 \n",
    "    comments：电视剧相关的评论的集合，集合中 是一系列 comment 对象 \n",
    "    url：电视剧对应的豆瓣页面的链接 \n",
    "    cover：电视剧的封面图链接 \n",
    "    \"\"\"\n",
    "    def __init__(self,Dict):\n",
    "        self.title = Dict['title']\n",
    "        self.rank = Dict['rate']\n",
    "        self.information = ''\n",
    "        self.introduction = '' \n",
    "        self.comments = []\n",
    "        self.url = Dict['url']\n",
    "        self.cover = Dict['cover']\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"\"\"《{title}》在豆瓣的评分：{rank}\\n相关信息：{information}\\n\n",
    "                    剧情简介：{introduction}\\n\n",
    "                \"\"\".format(title=self.title,rank = self.rank,information=self.information,\n",
    "                          introduction=self.introduction)\n",
    "    def showinformation(self):\n",
    "        #print(self.title)\n",
    "        pass\n",
    "    def getInformation(self,headers,downloadAllComments = True,Turns= 5):\n",
    "        \n",
    "        r = requests.get(self.url,headers=headers)\n",
    "        soup = BeautifulSoup(r.text,'html.parser')\n",
    "        self.information = soup.find('div',id='info').text\n",
    "        self.introduction = soup.find('div',class_='related-info').text\n",
    "        if downloadAllComments == True:    \n",
    "            firstStep = soup.find('div',id='comments-section').find('span',class_='pl')\n",
    "            href = firstStep.find('a')['href']\n",
    "            \n",
    "            num = int(re.search(r'[\\d]+',firstStep.text).group(0))#获取总评论数、\n",
    "            #避免评论太多爬取时间过长\n",
    "            if Turns:\n",
    "                RequestsTimes = Turns \n",
    "            else:\n",
    "                RequestsTimes = math.ceil(num/20)\n",
    "                \n",
    "            for once in range(RequestsTimes):\n",
    "                start = once*20 \n",
    "                commentsUrl = href +'&start={}&limit=20&sort=new_score&percent_type='.format(start)\n",
    "                response = requests.get(commentsUrl,headers=headers)\n",
    "                soup1 = BeautifulSoup(response.text,'html.parser')\n",
    "                time.sleep(0.1)\n",
    "                try:\n",
    "                    commentsList = soup1.find('div',class_= 'mod-bd').find_all('div',class_='comment-item')\n",
    "                    for com in commentsList:\n",
    "                        stars = com.find('span',class_=re.compile('rating'))['title']\n",
    "                        Time = com.find('span',class_='comment-time')['title']\n",
    "                        content=com.find('span',class_='short').text\n",
    "                        vote = com.find('span',class_='votes').text\n",
    "                        COM = comment(stars=stars,Time=Time,content=content,vote=vote)\n",
    "                        self.comments.append(COM)\n",
    "                except:\n",
    "                    break \n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "        \"\"\"\n",
    "        commentsList =soup.find('div',id='hot-comments').find_all('div',class_='comment-item')\n",
    "        for com in commentsList:            \n",
    "            stars = com.find('span',class_=re.compile('rating'))['title']\n",
    "            time = com.find('span',class_='comment-time')['title']\n",
    "            content=com.find('span',class_='short').text\n",
    "            COM = comment(stars=stars,time=time,content=content) \n",
    "            self.comments.append(COM)\n",
    "\n",
    "        \n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class doubanSpider(object):\n",
    "    \"\"\"\n",
    "    豆瓣爬虫，是一个爬取豆瓣电视剧的内容的爬虫，其主要 爬取每一个电视剧的相关介绍及其评论（目前只有热门评论） \n",
    "    参数： \n",
    "    nums：想要爬取的电视剧总数 \n",
    "    \n",
    "    属性：\n",
    "    head ： 爬虫的头部信息 \n",
    "    url\n",
    "    TVList：爬虫获取的所有电视剧的集合，其中的每一项都是tv对象\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,nums):\n",
    "        self.Totalnums = nums\n",
    "        self.head = {\n",
    "  'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.113 Safari/537.36',\n",
    "  'Cookie': 'bid=\"bSK74mb3o0U\"; __gads=ID=81316fb8c6d49141:T=1581057714:S=ALNI_MY6rZqhAl57J0nESAyAyXvRgncNxA; douban-fav-remind=1; ll=\"108309\"; gr_user_id=9d59bb85-5727-4d65-b377-18d1313eb1ca; _vwo_uuid_v2=DA4049E15DA0964D449EC0FD2852882E7|f98f0a5b2d5e82a5c914783eb1008f84; __utmz=30149280.1587041896.8.4.utmcsr=google|utmccn=(organic)|utmcmd=organic|utmctr=(not%20provided); viewed=\"26022002\"; ap_v=0,6.0; __utma=30149280.549375521.1581057714.1587041896.1587105876.9; __utmc=30149280; __utmb=30149280.10.10.1587105876; _pk_ses.100001.4cf6=*; __utma=223695111.1522244183.1587106246.1587106246.1587106246.1; __utmb=223695111.0.10.1587106246; __utmc=223695111; __utmz=223695111.1587106246.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none); __yadk_uid=5hqk227SXsAs0mKJ4P6VisbXsOP8syIv; _pk_id.100001.4cf6=76d2769dc72693ea.1587106246.1.1587106446.1587106246.'\n",
    "}\n",
    "        self.url = \"https://movie.douban.com/j/search_subjects?type=tv&tag=热门&sort=recommend&page_limit=20&page_start={}\"\n",
    "        self.TVList = []\n",
    "        \n",
    "    def get_List(self):\n",
    "        \n",
    "        TotalTurn = math.ceil(self.Totalnums/20)\n",
    "        \n",
    "        index1 = TotalTurn*20 #总的需要下载的数目\n",
    "        for startNum in range(TotalTurn):\n",
    "            start = startNum*20\n",
    "             \n",
    "            url = self.url.format(start)\n",
    "            r= requests.get(url,headers=self.head)\n",
    "            List = r.json()['subjects']\n",
    "            for i in List:\n",
    "                TV = tv(i)\n",
    "                TV.getInformation(self.head)\n",
    "                self.TVList.append(TV)\n",
    "                time.sleep(0.5)\n",
    "                print('正在爬取：<{}>的相关信息 '.format(TV.title)) \n",
    "            index2 = start+20 # 记录已经下载了多少数据\n",
    "            print('已经完成了{:.2f}%'.format(index2/index1))\n",
    "    def saveComments(self):\n",
    "        starsList = []\n",
    "        contentList = []\n",
    "        timeList = []\n",
    "        voteList = []\n",
    "        sourceList = []\n",
    "        for tv in self.TVList:\n",
    "            source = tv.title\n",
    "            for comment in tv.comments:\n",
    "                starsList.append(comment.stars)\n",
    "                contentList.append(comment.content)\n",
    "                timeList.append(comment.Time)\n",
    "                voteList.append(comment.vote)\n",
    "                sourceList.append(source)\n",
    "                \n",
    "        out = pd.DataFrame(zip(starsList,contentList,timeList,voteList,sourceList),\n",
    "                     columns=['评分','评论内容','评论时间','点赞数','电视剧名'])\n",
    "        out.to_csv('./comments.csv',encoding='utf-8')\n",
    "        \n",
    "    def comments_to_mysql(self):\n",
    "        mysql_config ={\n",
    "            'host':'localhost',\n",
    "            'port':3306,\n",
    "            'user':'root',\n",
    "            'password':'iamyourtf618',\n",
    "            'charset':'utf8mb4'\n",
    "        }\n",
    "        #创建'douban'数据库\n",
    "        create_database = \"\"\"CREATE DATABASE IF NOT EXISTS douban DEFAULT \n",
    "        CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci\"\"\"\n",
    "    \n",
    "        self.mysql_create_database(mysql_config,create_database)\n",
    "        \n",
    "        #创建'书名'表\n",
    "        create_table=\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS book(\n",
    "                id varchar(20) NOT NULL,\n",
    "                title varchar(30),\n",
    "                rank varchar(30),\n",
    "                )ENGINE=InnoDB DEFAULT CHARSET=utf8mb4\n",
    "        \"\"\"\n",
    "        self.mysql_create_table(mysql_config,create_table)\n",
    "    def mysql_create(self,connection,sql):\n",
    "        \"\"\"创建MySQL数据库或表\"\"\"\n",
    "        try:\n",
    "            with connection.cursor() as cursor:\n",
    "                cursor.execute(sql)\n",
    "        finally:\n",
    "            connection.close()\n",
    "\n",
    "    def mysql_create_database(self,mysql_config,sql):\n",
    "        \"\"\"创建MySQL数据库\"\"\"\n",
    "        try:\n",
    "            import pymysql\n",
    "        except ImportError:\n",
    "            sys.exit(u'系统中可能没有安装pymysql库，请先运行 pip install pymysql ，再运行程序')\n",
    "        try:\n",
    "            #if self.mysql_config:\n",
    "                #mysql_config = self.mysql_config\n",
    "            connection = pymysql.connect(**mysql_config)\n",
    "            self.mysql_create(connection,sql)\n",
    "        except pymysql.OperationalError:\n",
    "            sys.exit(u'系统中可能没有安装或者正确配置MySQL数据库，请先更具系统环境安装或配置MySQL，在再运行程序 ')\n",
    "    def mysql_create_table(self,mysql_config,sql):\n",
    "        \"\"\"创建MySQL表\"\"\"\n",
    "        import pymysql\n",
    "        \n",
    "        mysql_config['db']='douban'\n",
    "        connection = pymysql.connect(**mysql_config)\n",
    "        self.mysql_create(connectionn,sql)\n",
    "        \n",
    "    def mysql_insert(self,mysql_config,table):\n",
    "        \"\"\"向MySQL表插入或更新数据\"\"\"\n",
    "        #keys = ','.join()\n",
    "        keys = 'rank,title'\n",
    "        \n",
    "        mysql_config['db'] = 'douban'\n",
    "        connection = pymysql.connect(**mysql_config)\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        sql = \"\"\"INSERT INTO {table}({keys}) VALUES ({values}) \n",
    "        ON DUPLICATE KEY UPDATE\"\"\".format(table = table,\n",
    "                                         keys = keys,\n",
    "                                         values = values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在爬取：<我是余欢水>的相关信息 \n",
      "正在爬取：<龙岭迷窟>的相关信息 \n",
      "正在爬取：<十八年后的终极告白>的相关信息 \n",
      "正在爬取：<口罩猎人>的相关信息 \n",
      "正在爬取：<重生>的相关信息 \n",
      "正在爬取：<清平乐>的相关信息 \n",
      "正在爬取：<猎狐>的相关信息 \n",
      "正在爬取：<国王：永远的君主>的相关信息 \n",
      "正在爬取：<杀死伊芙 第三季>的相关信息 \n",
      "正在爬取：<夫妻的世界>的相关信息 \n",
      "正在爬取：<鬓边不是海棠红>的相关信息 \n",
      "正在爬取：<民国奇探>的相关信息 \n",
      "正在爬取：<机智医生生活>的相关信息 \n",
      "正在爬取：<叹息桥>的相关信息 \n",
      "正在爬取：<美国夫人>的相关信息 \n",
      "正在爬取：<冰糖炖雪梨>的相关信息 \n",
      "正在爬取：<陈情令>的相关信息 \n",
      "正在爬取：<西部世界 第三季>的相关信息 \n",
      "正在爬取：<那个男人的记忆法>的相关信息 \n",
      "正在爬取：<请回答1988>的相关信息 \n",
      "已经完成了1.00%\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t= doubanSpider(nums = 5)\n",
    "t.get_List()\n",
    "t.saveComments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.saveComments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
